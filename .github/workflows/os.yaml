name: Daily Trending Projects

on:
  # Triggers the workflow on push or pull request events but only for the "main" branch
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  scrape_trending_projects:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests
      - name: Scrape trending projects
        run: |
          import os
          from bs4 import BeautifulSoup
          import requests

          url = "https://github.com/trending?since=daily"
          response = requests.get(url)
          soup = BeautifulSoup(response.content, 'html.parser')
          trending_projects_div = soup.find('div', {'class': 'Box'})
          project_data = []

          for project in trending_projects_div.find_all('a'):
            project_name = project.text.strip()
            project_description = project.find_next_sibling('p').text.strip()
            project_url = 'https://github.com' + project.get('href')

            if (project_name, project_description) not in project_data:
              project_data.append((project_name, project_description, project_url))

          html_content = """
            <html>
              <head>
                <title>Trending Projects</title>
              </head>
              <body>
                <h1>Trending Projects</h1>
                """
          for project_name, project_description, project_url in project_data:
            html_content += f"""
              <p>{project_name}</p>
              <p>{project_description}</p>
              <p><a href="{project_url}">View on GitHub</a></p>
              <hr />
              """
          html_content += """
              </body>
            </html>
            """

          with open('.trending_projects.html', 'w') as f:
            f.write(html_content)
